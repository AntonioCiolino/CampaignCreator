"""Reformat chat_messages table for JSON history

Revision ID: <your_new_revision_id_here>
Revises: <id_of_previous_migration_script_here>
Create Date: YYYY-MM-DD HH:MM:SS.MS

"""
from alembic import op
import sqlalchemy as sa
# If using PostgreSQL and prefer JSONB:
# from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = '<your_new_revision_id_here>'
down_revision = '<id_of_previous_migration_script_here>' # e.g. 'manual_002_add_chat_messages_table' or your latest
branch_labels = None
depends_on = None


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###

    # Add new columns
    op.add_column('chat_messages', sa.Column('user_id', sa.Integer(), nullable=True)) # Will be made non-nullable after potential data migration
    op.add_column('chat_messages', sa.Column('conversation_history', sa.JSON(), nullable=True)) # Will be made non-nullable, default server '[]'
    # For PostgreSQL, you might prefer JSONB and a specific server default:
    # op.add_column('chat_messages', sa.Column('conversation_history', postgresql.JSONB(astext_type=sa.Text()), server_default=sa.text("'[]'::jsonb"), nullable=False))
    op.add_column('chat_messages', sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.func.now(), nullable=False))

    # If you need to update existing rows to have a default user_id before making user_id non-nullable:
    # Example: Update existing rows to a placeholder user_id (e.g., 1, if that user exists)
    # op.execute("UPDATE chat_messages SET user_id = 1 WHERE user_id IS NULL")
    # Then alter column to be non-nullable
    op.alter_column('chat_messages', 'user_id', nullable=False)

    # If you need to update existing rows for conversation_history (data migration)
    # This is where you would add custom SQL or Python logic if you are migrating existing
    # individual text/sender/timestamp messages into the new JSON array format.
    # For example (conceptual, actual logic depends on your data and needs):
    # conn = op.get_bind()
    # results = conn.execute(sa.text("SELECT id, character_id, user_id, text, sender, timestamp FROM old_chat_messages_backup")).fetchall()
    # for r in results:
    #    op.execute(sa.text(f"UPDATE chat_messages SET conversation_history = json_array_append(conversation_history, '$.new_message') WHERE id = {r.id}"))
    # After data migration, set conversation_history to non-nullable
    op.alter_column('chat_messages', 'conversation_history', nullable=False, server_default='[]')


    op.create_index(op.f('ix_chat_messages_user_id'), 'chat_messages', ['user_id'], unique=False)
    op.create_foreign_key(
        'fk_chat_messages_user_id_users',
        'chat_messages', 'users',
        ['user_id'], ['id']
    )

    # Create UniqueConstraint for (character_id, user_id)
    op.create_unique_constraint('uq_character_user_conversation', 'chat_messages', ['character_id', 'user_id'])

    # Drop old columns AFTER data migration (if any)
    # Ensure you have backed up or migrated data from these columns if needed.
    # Dropping index for old timestamp column if it exists (common default name)
    try:
        op.drop_index(op.f('ix_chat_messages_timestamp'), table_name='chat_messages')
    except Exception as e:
        print(f"Could not drop index ix_chat_messages_timestamp (it might not exist): {e}")

    op.drop_column('chat_messages', 'timestamp')
    op.drop_column('chat_messages', 'text')
    op.drop_column('chat_messages', 'sender')

    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###

    # Re-add old columns
    # Note: Data restoration from JSON to individual columns is complex and not handled here.
    # This downgrade path primarily restores schema, not necessarily data state.
    op.add_column('chat_messages', sa.Column('sender', sa.VARCHAR(), autoincrement=False, nullable=True)) # Was False
    op.add_column('chat_messages', sa.Column('text', sa.TEXT(), autoincrement=False, nullable=True))   # Was False
    op.add_column('chat_messages', sa.Column('timestamp', sa.DATETIME(timezone=True), server_default=sa.text('(CURRENT_TIMESTAMP)'), autoincrement=False, nullable=True)) # Was False

    # Make them non-nullable after potential (manual) data population if needed
    op.alter_column('chat_messages', 'sender', nullable=False)
    op.alter_column('chat_messages', 'text', nullable=False)
    op.alter_column('chat_messages', 'timestamp', nullable=False)

    # Recreate index for timestamp if it was part of your old schema
    op.create_index(op.f('ix_chat_messages_timestamp'), 'chat_messages', ['timestamp'], unique=False)

    # Drop new constraints and columns
    op.drop_constraint('uq_character_user_conversation', 'chat_messages', type_='unique')
    op.drop_constraint('fk_chat_messages_user_id_users', 'chat_messages', type_='foreignkey')
    op.drop_index(op.f('ix_chat_messages_user_id'), table_name='chat_messages')

    op.drop_column('chat_messages', 'updated_at')
    op.drop_column('chat_messages', 'conversation_history')
    op.drop_column('chat_messages', 'user_id')
    # ### end Alembic commands ###
